{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "807a913e-582f-443b-a458-4d424b910a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "634d09cb-ddbf-473e-a5f3-5d9433a463fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extraction Phase, where the data is loaded form the internet or other sources\n",
    "train = tfds.load(name = \"fashion_mnist\", split = \"train[:80%]\", as_supervised = True) # simple way to load tfds datasets and make custom splits on the data\n",
    "validation = tfds.load(name = \"fashion_mnist\", split = \"train[80%:90%]\", as_supervised = True) \n",
    "test = tfds.load(name = \"fashion_mnist\", split = \"train[90%:]\", as_supervised = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "29dfd799-fff8-490d-bfbd-55e6ac40568c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transorm Phase where the data is batched and augmented to prepare it for training\n",
    "## Batch and Augment data before training\n",
    "# Augmentation function\n",
    "def augment_images(image, label):\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    image = tf.image.random_flip_left_right(image)     # random horizontal flip\n",
    "    image = tf.image.random_flip_up_down(image)        # random vertical flip\n",
    "    image = tf.image.random_brightness(image, max_delta=0.2)  # brightness jitter\n",
    "    image = tf.image.random_contrast(image, 0.8, 1.2)  # contrast jitter\n",
    "    return image, label\n",
    "\n",
    "# Batch size\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Apply map → shuffle → batch → prefetch\n",
    "train = (train.map(augment_images)\n",
    "              .shuffle(10000)\n",
    "              .batch(BATCH_SIZE)\n",
    "              )\n",
    "\n",
    "validation = (validation.map(augment_images)\n",
    "                       .batch(BATCH_SIZE)\n",
    "                       )\n",
    "\n",
    "test = (test.map(augment_images)\n",
    "             .batch(BATCH_SIZE)\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b73c68b-6bcd-4968-8cdf-062f132e9904",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yusuf Solomon\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 6ms/step - accuracy: 0.7324 - loss: 0.7519 - val_accuracy: 0.8072 - val_loss: 0.5474\n",
      "Epoch 2/5\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.8015 - loss: 0.5584 - val_accuracy: 0.8180 - val_loss: 0.4922\n",
      "Epoch 3/5\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.8171 - loss: 0.5136 - val_accuracy: 0.8440 - val_loss: 0.4296\n",
      "Epoch 4/5\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8257 - loss: 0.4876 - val_accuracy: 0.8422 - val_loss: 0.4347\n",
      "Epoch 5/5\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8320 - loss: 0.4697 - val_accuracy: 0.8455 - val_loss: 0.4296\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x247dd54c890>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Because of the way the data was loaded, the shape is now 28,28,1 \n",
    "## And this has to be specified in the model that we train on this data\n",
    "## Load Phase, where the data is Loaded into the model for training\n",
    "\n",
    "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(input_shape=(28,28,1)), # Specify correct input shape\n",
    "                                    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "                                    tf.keras.layers.Dropout(0.2),\n",
    "                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])\n",
    "\n",
    "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "model.fit(train, validation_data = validation, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "965e5620-bbc7-4812-9242-797a809040df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Variant folder C:\\Users\\Yusuf Solomon\\tensorflow_datasets\\horses_or_humans\\3.0.0 has no dataset_info.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\\Users\\Yusuf Solomon\\tensorflow_datasets\\horses_or_humans\\3.0.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d7db439eae345509a20f9a163010302",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f82a06a0a94f44548f968e79517a21b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c90655e97ad45e39427873deb7238f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/2 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fbb1971ec4941b69f735b76baaa5d3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c13593939393422796f04a5a193b277e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling C:\\Users\\Yusuf Solomon\\tensorflow_datasets\\horses_or_humans\\incomplete.DFF3WP_3.0.0\\horses_or_humans…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ed9865ca76d424a9ccf2d723af3a2a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test examples...: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c97f07d36c4441ea1a6aea67b8f93e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling C:\\Users\\Yusuf Solomon\\tensorflow_datasets\\horses_or_humans\\incomplete.DFF3WP_3.0.0\\horses_or_humans…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset horses_or_humans downloaded and prepared to C:\\Users\\Yusuf Solomon\\tensorflow_datasets\\horses_or_humans\\3.0.0. Subsequent calls will reuse this data.\u001b[0m\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yusuf Solomon\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 515ms/step - accuracy: 0.8315 - loss: 1.6417\n",
      "Epoch 2/10\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 489ms/step - accuracy: 0.9387 - loss: 0.1974\n",
      "Epoch 3/10\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 497ms/step - accuracy: 0.9776 - loss: 0.0822\n",
      "Epoch 4/10\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 546ms/step - accuracy: 0.9834 - loss: 0.0518\n",
      "Epoch 5/10\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 516ms/step - accuracy: 0.9464 - loss: 0.1723\n",
      "Epoch 6/10\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 471ms/step - accuracy: 0.9893 - loss: 0.0433\n",
      "Epoch 7/10\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 486ms/step - accuracy: 1.0000 - loss: 0.0011\n",
      "Epoch 8/10\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 475ms/step - accuracy: 1.0000 - loss: 8.7488e-05\n",
      "Epoch 9/10\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 466ms/step - accuracy: 1.0000 - loss: 4.8818e-05\n",
      "Epoch 10/10\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 480ms/step - accuracy: 1.0000 - loss: 3.4704e-05\n"
     ]
    }
   ],
   "source": [
    "## Optimizing ETL process in tensorflow\n",
    "## we can do this by performing the Extraction and the loading in parallel, saving time and resources\n",
    "## We do not keep our other resources idle, while we are training. We extrac and train in parallel\n",
    "data = tfds.load('horses_or_humans', split='train', as_supervised=True)\n",
    "train_batches = data.shuffle(100).batch(10)\n",
    "model = tf.keras.models.Sequential([tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(300, 300, 3)),\n",
    "                                    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "                                    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
    "                                    tf.keras.layers.MaxPooling2D(2,2),\n",
    "                                    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "                                    tf.keras.layers.MaxPooling2D(2,2),\n",
    "                                    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "                                    tf.keras.layers.MaxPooling2D(2,2),\n",
    "                                    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "                                    tf.keras.layers.MaxPooling2D(2,2),\n",
    "                                    tf.keras.layers.Flatten(),\n",
    "                                    tf.keras.layers.Dense(512, activation='relu'),\n",
    "                                    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "                                   ])\n",
    "\n",
    "model.compile(optimizer='Adam', loss='binary_crossentropy',metrics=['accuracy'])\n",
    "history = model.fit(train_batches, epochs=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
